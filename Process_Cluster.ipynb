{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import glob\n",
    "from dateutil import parser\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_date = ''\n",
    "last_time = ''\n",
    "\n",
    "def readLogFileWithGlob(globString, date, logTimeStampStr):\n",
    "    logfile = pd.read_csv(glob.glob(globString)[0], header=None)\n",
    "    logfile[0] = logfile[0].apply(lambda x: int(getTimeStamp(logTimeStampStr, x.strip().split(' ')[0]).timestamp()))\n",
    "    logfile[2] = logfile[2].apply(lambda x: x.strip().split(' ')[2].strip())\n",
    "    logfile[2] = logfile[2].apply(lambda x: float(x))\n",
    "    logfile[3] = logfile[3].apply(lambda x: float(x))\n",
    "    logfile[4] = logfile[4].apply(lambda x: float(x))\n",
    "    logfile.columns = ['time', 'users', 'load1','load5', 'load15']\n",
    "    logfile = logfile.drop(columns=['users']);\n",
    "    return logfile\n",
    "\n",
    "def cleanSparkgenLine(line):\n",
    "    splitted = line.split(' ')\n",
    "    global last_date\n",
    "    global last_time\n",
    "    if(len(splitted) < 2):\n",
    "        return [last_date, last_time, line]\n",
    "    last_date = splitted[0].strip()\n",
    "    last_time = splitted[1].strip()\n",
    "    return [last_date, last_time, ' '.join(splitted[2:len(splitted)])]\n",
    "\n",
    "def groupLinesByThree(lines):\n",
    "    startTime = parser.parse(lines[0][1])\n",
    "    endTime = parser.parse(lines[2][1])\n",
    "    duration = (endTime - startTime).total_seconds()\n",
    "\n",
    "    result = lines[2][2]\n",
    "    if \"lenet5\" in lines[0][2]:\n",
    "        application = \"lenet5\"\n",
    "    else:\n",
    "        application = \"bi-rnn\"\n",
    "    return [startTime, endTime, duration, result, application]\n",
    "\n",
    "def readSparkGen(logTimeStampStr, prefix = '.'):\n",
    "    filename = prefix + '/' + 'sparkgen'\n",
    "    sparkgenDate = ''\n",
    "    first = True\n",
    "    with open(filename) as f:\n",
    "        content = f.readlines()\n",
    "    # you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "    content = [cleanSparkgenLine(x) for x in content]\n",
    "    \n",
    "    df_sparkgen = pd.DataFrame(content)\n",
    "    df_sparkgen.columns = ['date', 'time', 'content']\n",
    "    correctLine = df_sparkgen[\"content\"].str.contains(('true,'))\n",
    "    valid = df_sparkgen[\"content\"].str.startswith(('[LOG')) | df_sparkgen[\"content\"].str.contains(('true,'))\n",
    "    correctData = df_sparkgen[correctLine]\n",
    "    \n",
    "    total = 0\n",
    "    experiments = pd.DataFrame(columns=['start', 'end', 'duration', 'result', 'application'])\n",
    "    for index, row in correctData.iterrows():\n",
    "        splitted = row['content'].split(',')\n",
    "        if first:\n",
    "            first = False\n",
    "            sparkgenDate = row['date']\n",
    "        total += 1\n",
    "        duration = PFTS_Old(splitted[5]) - PFTS_Old(splitted[4])\n",
    "        if(duration < 0):\n",
    "            duration = PFTS_alt(splitted[5]) - PFTS_alt(splitted[4])\n",
    "        new_start_time = (parser.parse(row['date'] + \" \" + row['time'] ) - datetime.timedelta(seconds=duration)).strftime(\"%H:%M:%S\")\n",
    "        new_end_time = row['time']\n",
    "        experiments.loc[total] =[ getTimeStamp(logTimeStampStr, new_start_time).timestamp(), getTimeStamp(logTimeStampStr, new_end_time).timestamp(), duration, row['content'], 'lenet5']\n",
    "    return [experiments, sparkgenDate]\n",
    "\n",
    "def filter_log_row(row, exp):\n",
    "    res = (exp['start'] <= row) & (exp['end'] >= row)\n",
    "    numValid = (res).sum()\n",
    "    if numValid > 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def PFTS_logs(str):\n",
    "    epochTime = str\n",
    "    dt = datetime.datetime.fromtimestamp(epochTime, datetime.timezone.utc)\n",
    "    str_time = dt.strftime(\"%H:%M:%S\")\n",
    "    newEpoch = int(time.mktime(parser.parse(str_time).timetuple()))\n",
    "    return newEpoch                         \n",
    "\n",
    "def PFTS_Old(str):\n",
    "    return int(round(float(str)))\n",
    "\n",
    "def PFTS_alt(str):\n",
    "    epochTime = PFTS_Old(str) + 7200\n",
    "    dt = datetime.datetime.fromtimestamp(epochTime, datetime.timezone.utc)\n",
    "    str_time = dt.strftime(\"%H:%M:%S\")\n",
    "    newEpoch = int(time.mktime(parser.parse(str_time).timetuple()))\n",
    "    return newEpoch \n",
    "\n",
    "def PFTS(str):\n",
    "    epochTime = PFTS_Old(str)\n",
    "    dt = datetime.datetime.fromtimestamp(epochTime, datetime.timezone.utc)\n",
    "    str_time = dt.strftime(\"%H:%M:%S\")\n",
    "    newEpoch = int(time.mktime(parser.parse(str_time).timetuple()))\n",
    "    return newEpoch         \n",
    "    \n",
    "def filterSlaveDataFrame(df, exp):\n",
    "    filtered_rows = df['time'].apply(lambda x: filter_log_row(x, exp))\n",
    "    return df[filtered_rows]\n",
    "\n",
    "def readAllSlaveLogs(date, logTimeStampStr, prefix = '.'):\n",
    "    df1 = readLogFileWithGlob(prefix + '/' +'*_1.log', date, logTimeStampStr)\n",
    "    df2 = readLogFileWithGlob(prefix + '/' +'*_2.log', date, logTimeStampStr)\n",
    "    df3 = readLogFileWithGlob(prefix + '/' +'*_3.log', date, logTimeStampStr)\n",
    "    df4 = readLogFileWithGlob(prefix + '/' +'*_4.log', date, logTimeStampStr)\n",
    "    return {'log1': df1, 'log2': df2, 'log3': df3, 'log4': df4}\n",
    "\n",
    "def filterLogs(logs, exp):\n",
    "    filtered_df1 = filterSlaveDataFrame(logs['log1'], exp)\n",
    "    filtered_df2 = filterSlaveDataFrame(logs['log2'], exp)\n",
    "    filtered_df3 = filterSlaveDataFrame(logs['log3'], exp)\n",
    "    filtered_df4 = filterSlaveDataFrame(logs['log4'], exp)\n",
    "    return {'log1': filtered_df1, 'log2': filtered_df2, 'log3': filtered_df3, 'log4': filtered_df4}\n",
    "\n",
    "def bestDate(l):\n",
    "    if not pd.isnull(l[0]):\n",
    "        return l[0]\n",
    "    if not pd.isnull(l[1]):\n",
    "        return l[1]\n",
    "    if not pd.isnull(l[2]):\n",
    "        return l[2]\n",
    "    if not pd.isnull(l[3]):\n",
    "        return l[3]\n",
    "    \n",
    "def averageWithNaN(values):\n",
    "    return np.nanmean(values)\n",
    "\n",
    "def averageLogSet(logs):\n",
    "    tmp1 = logs['log1']\n",
    "    tmp1.columns = ['time-1','load1-1', 'load5-1', 'load15-1']\n",
    "    tmp2 = logs['log2']\n",
    "    tmp2.columns = ['time-2','load1-2', 'load5-2', 'load15-2']\n",
    "    tmp3 = logs['log3']\n",
    "    tmp3.columns = ['time-3','load1-3', 'load5-3', 'load15-3']\n",
    "    tmp4 = logs['log4']\n",
    "    tmp4.columns = ['time-4','load1-4', 'load5-4', 'load15-4']\n",
    "\n",
    "    combined = pd.concat([tmp1, tmp2, tmp3, tmp4], axis=1)\n",
    "\n",
    "    averaged = pd.DataFrame(columns=['time', 'avg_load1', 'avg_load5', 'avg_load15'])\n",
    "    for index, row in combined.iterrows():\n",
    "        time = bestDate([row['time-1'],row['time-2'], row['time-3'], row['time-4']])\n",
    "        avg_load1 = averageWithNaN([row['load1-1'], row['load1-2'], row['load1-3'], row['load1-4']])\n",
    "        avg_load5 = averageWithNaN([row['load5-1'], row['load5-2'], row['load5-3'], row['load5-4']])\n",
    "        avg_load15 = averageWithNaN([row['load15-1'], row['load15-2'], row['load15-3'], row['load15-4']])\n",
    "        averaged.loc[index] = [time, avg_load1, avg_load5, avg_load15]\n",
    "    return averaged\n",
    "\n",
    "def correctForDD(name, logs):\n",
    "    # Parse name\n",
    "    splitted = name.split('_')\n",
    "    fast_slaves = int(splitted[3])\n",
    "    slow_slaves = 4 - fast_slaves\n",
    "    logList = ['log1','log2','log3','log4']\n",
    "    fast_logs = logList[0:fast_slaves]\n",
    "    slow_logs = logList[fast_slaves:4]\n",
    "    newLogs = {}\n",
    "    for logName in slow_logs:\n",
    "        newLogs[logName] = correctDFForDD(logs[logName], 0.75)\n",
    "    for logName in fast_logs:\n",
    "        newLogs[logName] = correctDFForDD(logs[logName], 0.5)\n",
    "    return newLogs\n",
    "\n",
    "def correctDFForDD(df, factor):\n",
    "    result = df.copy()\n",
    "    result['load1'] = result['load1'].apply(lambda x :x)\n",
    "    result['load5'] = result['load5'].apply(lambda x : x)\n",
    "    result['load15'] = result['load15'].apply(lambda x : x)\n",
    "    return result\n",
    "\n",
    "def extractLoadForTimeRange(start, end, loadData, resolution='avg_load1'):\n",
    "    mask = (loadData['time'] >= start) & (loadData['time'] <= end)\n",
    "    meanDataForExp = loadData[mask].iloc[1:].mean(axis = 0) \n",
    "    return meanDataForExp\n",
    "    \n",
    "def getLogDataPerRow(row, data):\n",
    "    \n",
    "    startTime = row['start']\n",
    "    endTime = row['end']\n",
    "    return extractLoadForTimeRange(startTime, endTime, data)\n",
    "    \n",
    "def getIndividualExperiments(sparkgen, data):\n",
    "    combinedSparkgen = pd.DataFrame(columns=['start','end','duration','result','application','avg_load1', 'avg_load5', 'avg_load15'])\n",
    "    for index, row in sparkgen.iterrows():\n",
    "        meanValues = getLogDataPerRow(row, data)\n",
    "        newRow = np.append(row.values, meanValues)\n",
    "        combinedSparkgen.loc[index] = newRow\n",
    "    return combinedSparkgen\n",
    "\n",
    "def extractLoadForTimeRange(start, end, loadData, resolution='avg_load1'):\n",
    "    mask = (loadData['time'] >= start) & (loadData['time'] <= end)\n",
    "    # Drop the first row\n",
    "    meanDataForExp = loadData[mask].iloc[1:].mean(axis = 0) \n",
    "    return meanDataForExp\n",
    "    \n",
    "def getLogDataPerRow(row, data):\n",
    "    \n",
    "    startTime = row['start']\n",
    "    endTime = row['end']\n",
    "    return extractLoadForTimeRange(startTime, endTime, data)\n",
    "    \n",
    "def getIndividualExperiments(expStruct, dset='filtered_data'):\n",
    "    data = expStruct[dset]['averaged'].copy()\n",
    "    sparkgen = expStruct['sparkgen']\n",
    "    tl = expStruct['typesList']\n",
    "    combinedSparkgen = pd.DataFrame(columns=['cpu_ratio_of_fast', 'ram_of_all_nodes', 'number_of_fast', 'start','end','duration','result','application','avg_load1', 'avg_load5', 'avg_load15'])\n",
    "    for index, row in sparkgen.iterrows():\n",
    "        meanValues = getLogDataPerRow(row, data)    \n",
    "        combinedSparkgen = combinedSparkgen.append({\n",
    "            'cpu_ratio_of_fast' : tl[0], \n",
    "            'ram_of_all_nodes'  : tl[1], \n",
    "            'number_of_fast' : tl[2], \n",
    "            'start' : row['start'],\n",
    "            'end': row['end'],\n",
    "            'duration': row['duration'],\n",
    "            'result': row['result'],\n",
    "            'application': row['application'],\n",
    "            'avg_load1' : meanValues['avg_load1'], \n",
    "            'avg_load5': meanValues['avg_load5'], \n",
    "            'avg_load15': meanValues['avg_load15']\n",
    "        }, ignore_index=True)\n",
    "    return combinedSparkgen\n",
    "\n",
    "def extractLoadForTimeRange(start, end, loadData, resolution='avg_load1'):\n",
    "    mask = (loadData['time'] >= start) & (loadData['time'] <= end)\n",
    "    # Drop the first row\n",
    "    meanDataForExp = loadData[mask].iloc[1:].mean(axis = 0) \n",
    "    return meanDataForExp\n",
    "    \n",
    "def getLogDataPerRow(row, data):\n",
    "    startTime = row['start']\n",
    "    endTime = row['end']\n",
    "    return extractLoadForTimeRange(startTime, endTime, data)\n",
    "    \n",
    "def getIndividualExperiments(sparkgen, data, tl):\n",
    "    combinedSparkgen = pd.DataFrame(columns=['cpu_ratio_of_fast', 'ram_of_all_nodes', 'number_of_fast', 'start','end','duration','result','application','avg_load1', 'avg_load5', 'avg_load15','norml1', 'norml5', 'norml15'])\n",
    "    for index, row in sparkgen.iterrows():\n",
    "        meanValues = getLogDataPerRow(row, data.copy())    \n",
    "        combinedSparkgen = combinedSparkgen.append({\n",
    "            'cpu_ratio_of_fast' : tl[0], \n",
    "            'ram_of_all_nodes'  : tl[1], \n",
    "            'number_of_fast' : tl[2], \n",
    "            'start' : row['start'],\n",
    "            'end': row['end'],\n",
    "            'duration': row['duration'],\n",
    "            'result': row['result'],\n",
    "            'application': row['application'],\n",
    "            'avg_load1' : meanValues['avg_load1'], \n",
    "            'avg_load5': meanValues['avg_load5'], \n",
    "            'avg_load15': meanValues['avg_load15'],\n",
    "            'norml1': meanValues['avg_load1'] / row['duration'],\n",
    "            'norml5': meanValues['avg_load5'] / row['duration'],\n",
    "            'norml15': meanValues['avg_load15'] / row['duration'],\n",
    "        }, ignore_index=True)\n",
    "    return combinedSparkgen\n",
    "\n",
    "def getTimeStamp(after, string):\n",
    "    day = datetime.datetime.utcfromtimestamp(after)\n",
    "    timestring = datetime.datetime.strptime(str(day.date())+\"T\"+string, \"%Y-%m-%dT%H:%M:%S\")\n",
    "    if timestring < day:\n",
    "        timestring += datetime.timedelta(days=1)\n",
    "\n",
    "    return timestring\n",
    "\n",
    "def ClusterDataSet(name, path):\n",
    "    logtimestamp = float(path.split('_')[0])\n",
    "    \n",
    "    sparkgenData = readSparkGen(logtimestamp, path)\n",
    "    sparkgen = sparkgenData[0]\n",
    "    logs_raw = readAllSlaveLogs(sparkgenData[1], logtimestamp, path)\n",
    "    logs_corrected = readAllSlaveLogs(sparkgenData[1], logtimestamp, path)\n",
    "    logs_corrected = correctForDD(name ,logs_corrected)\n",
    "    logs_filtered = readAllSlaveLogs(sparkgenData[1], logtimestamp, path)\n",
    "    logs_filtered = correctForDD(name ,logs_filtered)\n",
    "    lf = filterLogs(logs_filtered, sparkgen)\n",
    "    logs_raw_averaged = averageLogSet(logs_raw)\n",
    "    logs_corrected_averaged = averageLogSet(logs_corrected)\n",
    "    lf_averaged = averageLogSet(lf)\n",
    "    \n",
    "    cpu_ratio_of_fast = int(name.split('_')[0])\n",
    "    ram_of_all_nodes = int(name.split('_')[1])\n",
    "    number_of_fast = int(name.split('_')[2])\n",
    "    tl = [cpu_ratio_of_fast, ram_of_all_nodes, number_of_fast]\n",
    "\n",
    "    dataset = {\n",
    "        \"name\": name,\n",
    "        \"typesList\": tl,\n",
    "        \"raw_data\": {\"logs\": logs_raw, \"averaged\": logs_raw_averaged},\n",
    "        \"logs_corrected\": {\"logs\": logs_corrected, \"averaged\": logs_corrected_averaged},\n",
    "        \"filtered_data\": {\"logs\": lf, \"averaged\": lf_averaged},\n",
    "        \"sparkgen\": sparkgen,\n",
    "        \"sepExps\": getIndividualExperiments(sparkgen, lf_averaged, tl)\n",
    "              }\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load all experiments\n",
    "* Parse\n",
    "* Clean\n",
    "* Correlated loadvalues to experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_data = {}\n",
    "for filename in glob.iglob('*', recursive=True):\n",
    "    if not os.path.isfile(filename): # filter dirs\n",
    "        name = ''\n",
    "        if len(filename.split('.')) < 3:\n",
    "            name = '_'.join(filename.split('_')[1:5])\n",
    "        else:\n",
    "            name = filename.split('.')[2]\n",
    "        print(\">> Loading \" + name)\n",
    "        dataset = ClusterDataSet(name, filename)\n",
    "        experiment_data[name] = dataset\n",
    "print(\">> Finished loading all experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge all single experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allExps = []\n",
    "for key, exp in experiment_data.items():\n",
    "    allExps.append(exp['sepExps'])\n",
    "allExperiments =  pd.concat(allExps, ignore_index=True)\n",
    "allExperiments.to_csv('output_lenet5.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create experiment summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.DataFrame(columns=['CPU', 'RAM', 'Machine Ratio','Duration', 'Load'])\n",
    "idx = 0\n",
    "for key, exp in experiment_data.items():\n",
    "    avg_duration = exp['sparkgen']['duration'].mean()\n",
    "    loadavg = exp['filtered_data']['averaged'].mean()['avg_load1']\n",
    "    row = np.append(exp['typesList'], avg_duration)\n",
    "    row = np.append(row, exp['filtered_data']['averaged'].mean()['avg_load1'])\n",
    "    summary.loc[idx] = row\n",
    "    idx += 1\n",
    "summary['CPU'] = summary['CPU'].apply(int)\n",
    "summary['RAM'] = summary['RAM'].apply(int)\n",
    "summary['Machine Ratio'] = summary['Machine Ratio'].apply(int)\n",
    "summary.to_csv('summary.csv', sep='\\t')\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import researchpy as rp\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "import statsmodels.stats.multicomp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('output_lenet5.csv', delimiter='\\t')\n",
    "df.columns = ['index','CPU', 'RAM', 'Machine_Ratio', 'start', 'end', 'duration', 'result', 'application', 'load', 'avg_load5', 'avg_load15','norml1','norml5','norml15']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_load = ols('load ~ C(CPU)*C(RAM)*C(Machine_Ratio)', df).fit()\n",
    "model_duration = ols('duration ~ C(CPU)*C(RAM)*C(Machine_Ratio)', df).fit()\n",
    "\n",
    "# Seeing if the overall model is significant\n",
    "print(f\"Load Model: Overall model F({model_load.df_model: .0f},{model_load.df_resid: .0f}) = {model_load.fvalue: .3f}, p = {model_load.f_pvalue: .4f}\")\n",
    "print(f\"Duration Model: Overall model F({model_duration.df_model: .0f},{model_duration.df_resid: .0f}) = {model_duration.fvalue: .3f}, p = {model_duration.f_pvalue: .4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_load.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duration Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_duration.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Anova stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_load = sm.stats.anova_lm(model_load, typ= 2)\n",
    "res_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_duration = sm.stats.anova_lm(model_duration, typ= 2)\n",
    "res_duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write output of models to latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('anova_load.tex','w') as tf:\n",
    "    tf.write(res_load.to_latex(index=True))\n",
    "\n",
    "with open('model_load.tex','w') as tf:\n",
    "    tf.write(model_load.summary().as_latex())\n",
    "\n",
    "with open('anova_duration.tex','w') as tf:\n",
    "    tf.write(res_duration.to_latex(index=True))\n",
    "    \n",
    "with open('model_duration.tex','w') as tf:\n",
    "    tf.write(model_duration.summary().as_latex())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
